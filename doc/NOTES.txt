Data Capture Ingest Adaptor script - notes.



Input

DC18A - CSV file, one row of headers, one row of data.
DC18B - haven't received sample metadata yet.

Output
Metadata Interchange XML
HTML dataset description (a la Peter Sefton's idea of machine and human readable docs)
Register the object with FC via Catmandu


//// DECISION ////

Next step:

a "publish" script which does the following:

* copy the payload datastreams to web hosted locations
* add them (as URLs) to Fedora.
* get Damyata to serve the URLS from the filesystem

This may be the point at which I merge CoataGlue and Damyata.

CoataGlue => Dayadhvam?




* Problem - we can only add datastreams to Fedora as URLs if
  they are live (or it throws an internal server error).
  
So: copy datastream file to the appropriate web hosting 
directory under Damyata - which can then serve the URLs

Then put the URLs in as datastreams, so that they appear
when the landing page is up.



* Landing page - based on Fedora and Solr data

Each datastream URL on the landing page goes to -> 

* A Damyata pcid/ds URL which gets the file from /local/ /public/
  or whatever.
  
  so DM / local/$RCid/$dsid
               / ...
               \
        / public/$RCid2/$dsid...
        
        
  Anticipated problem with this is that we're going to get a very 
  large drectory in /local /public etc

Alternative:

* pass the datastreams into Fedora as files, and then have Damyata
  fetch the datastreams


Fedora Questions
----------------

- A separate file storage for published datasets.

- Keep the datastreams in Fedora's "native" file system, or point 
  to a location?
  
- How does the publication workflow work?

* Experiment creates data
* Data is placed in a UTSRDC::Source directory
* Scanned by UTSRDC and the following happens
   - Create an FC object for this dataset
   - Write out an XML file for ingest by ReDBox, which has
     the Fedora object's digital ID

* If the object is to be published immediately, do the following:

   - copy the dataset onto Fedora storage
 
* At some later state, the user may publish the
  object.  When they do, 
  
   - copy the dataset onto Fedora storage (Don't know how this is 
     going to work yet.)

* Later - the Perl wrapper around FC does the following
  when a request is made:
  
  datasets.uts.edu.au/datasets/FEDORAID
  
  - look up FEDORAID in the RDC, and check its publication status.
  
  - Not Published: return a 404
  - Internal Publication: if the request is from UTS, return the
      cover page
  - Publication: return the cover page


The Fedora ID is stored in the redbox record as a URL.




Bare Minimum metadata
---------------------

This is the minimum metadata set which will get written into
the repository and into ReDBox.

I need to separate the crosswalk-to-common-fields bit from the 
write-to-XML bit.  This is in Source::render_view - should work
like this

my $dc = $dataset->extract_metadata() # returns a hashref

my $xml = $dataset->redbox_xml()      # returns actual XML





Terminology
-----------

Source     - a data capture source eg DC18A, DC18B - with a single 
             metadata ingest format, harvest frequency etc
             
Converter  - Perl class which does the metadata scanning for a particular 
             delivery method (eg, folders with csv files in them)
             
Dataset    - Perl class representing a dataset - has an id and a metadata record	

ID  	-  - Mints IDs.  Deliberately decoupled from Converter


Classes
-------

UTSRDC     - 

UTSRDC::Converter
UTSRDC::Converter::FolderCSV

UTSRDC::Source

UTSRDC::Dataset

UTSRDC::ID::NaiveSequence
